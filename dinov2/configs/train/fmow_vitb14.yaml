MODEL:
  WEIGHTS: ''
train:
  batch_size_per_gpu: 32
  dataset_path: null
  centering: sinkhorn_knopp
  OFFICIAL_EPOCH_LENGTH: 1000
  saveckp_freq: 20
evaluation:
  eval_period_iterations: 5000000000
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.0
  ffn_layer: mlp
  block_chunks: 0
  pretrained_weights: /home/nikola.jovisic.ivi/nj/dinov2/dinov2_vitb14_reg4_pretrain.pth
teacher:
  momentum_teacher: 0.994
optim:
  epochs: 200
  weight_decay: 0.
  weight_decay_end: 0.
  base_lr: 1.0e-04  # learning rate for a batch size of 1024
  warmup_epochs: 10
  layerwise_decay: 1.0
crops:
  global_crops_size: 518
  local_crops_size: 98
  
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32

lora:
  type: lora
  rank: 32
  layers:
    - attn
  unfreeze_blocks: []
  unfreeze_embed: false
  unfreeze_cls_token: false
  unfreeze_norm: true
  attn_key: false
  attn_proj: false
