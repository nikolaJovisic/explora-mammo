MODEL:
  WEIGHTS: ''
train:
  batch_size_per_gpu: 32
  dataset_path: fmow:data_and_checkpoints/fmow_csvs/train_62classes.csv
  centering: sinkhorn_knopp
  OFFICIAL_EPOCH_LENGTH: 1000
evaluation:
  eval_period_iterations: 50000
student:
  arch: vit_giant2
  patch_size: 14
  drop_path_rate: 0.0
  ffn_layer: swiglufused
  block_chunks: 0
  pretrained_weights: data_and_checkpoints/dinov2_vitg14_pretrain.pth
teacher:
  momentum_teacher: 0.994
optim:
  epochs: 200
  weight_decay: 0.
  weight_decay_end: 0.
  base_lr: 1.0e-04  # learning rate for a batch size of 1024
  warmup_epochs: 10
  layerwise_decay: 1.0
crops:
  local_crops_size: 98


compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32

lora:
  type: lora
  rank: 32
  layers:
    - attn
  unfreeze_blocks: []
  unfreeze_embed: false
  unfreeze_cls_token: false
  unfreeze_norm: true
  attn_key: false
  attn_proj: false
